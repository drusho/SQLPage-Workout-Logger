{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b5d5818",
   "metadata": {},
   "source": [
    "# Project Documentation Generator: SQLPage Workout Logger\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Jupyter Notebook is an automated tool designed to generate a comprehensive suite of documentation for the **SQLPage Workout Logger** project. It inspects the project's database, file structure, and source code comments to produce a set of up-to-date markdown reports, fully formatted with YAML frontmatter for use in Obsidian.\n",
    "\n",
    "The notebook generates the following reports:\n",
    "1.  **Database Schema Report:** A detailed look at each table, its columns, and sample data.\n",
    "2.  **Folder Tree Report:** An ASCII representation of the project's file structure.\n",
    "3.  **SQL Comment Documentation:** A reference guide generated from Javadoc-style comments in your `.sql` files.\n",
    "4.  **Project Style Guide:** A static document outlining coding and documentation standards.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1.  [**Configuration**](#1-Configuration)\n",
    "2.  [**How to Use**](#2-How-to-Use)\n",
    "3.  [**Report Generation**](#3-Report-Generation)\n",
    "    - [Part A: Database Schema Report](#Part-A-Database-Schema-Report)\n",
    "    - [Part B: Folder Tree Report](#Part-B-Folder-Tree-Report)\n",
    "    - [Part C: SQL Documentation Report](#Part-C-SQL-Documentation-Report)\n",
    "    - [Part D: Project Style Guide](#Part-D-Project-Style-Guide)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Configuration\n",
    "\n",
    "All user-configurable settings are located in the first code cell. You must review and update these paths before running the notebook.\n",
    "\n",
    "- **`project_name`**: The base name used in report titles and filenames (e.g., `\"SQLPage - Workout Logger\"`).\n",
    "- **`db_path`**: The full or relative path to the SQLite database file (e.g., `'workouts.db'`).\n",
    "- **`source_folder_for_tree`**: The path to the project's root folder (e.g., `'./sqlpage'`), used for generating the folder tree and parsing SQL comments.\n",
    "- **`SAMPLE_ROWS`**: The number of sample data rows to include in the schema report.\n",
    "- **`export_folder`**: The primary destination folder for the generated markdown reports.\n",
    "- **`obsidian_folder`**: An optional second destination folder, such as an Obsidian vault. Leave empty (`\"\"`) to disable.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. How to Use\n",
    "\n",
    "1.  Update the variables in the **Configuration** code cell immediately following this section.\n",
    "2.  Run all cells in the notebook sequentially from top to bottom. Using your editor's \"Run All\" command is recommended.\n",
    "3.  The final report files will be saved to the location(s) specified in `export_folder` and `obsidian_folder`.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Report Generation\n",
    "\n",
    "The subsequent cells perform the automated documentation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6ab8b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba65b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 1: Import all the things! (necessary libraries)\n",
    "import sqlite3\n",
    "import os\n",
    "import datetime\n",
    "from IPython.display import display, Markdown\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8236c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 2: Configuration\n",
    "# This cell holds all the settings for your reports.\n",
    "\n",
    "# --- CORE CONFIGURATION ---\n",
    "# The project name used in titles and filenames\n",
    "project_name = \"SQLPage - Workout\"\n",
    "\n",
    "# Path to the database file to document\n",
    "db_path = '/Volumes/Public/Container_Settings/sqlpage/www/workouts.db'\n",
    "\n",
    "# Path to the project folder to map into an ASCII tree\n",
    "source_folder_for_tree = '/Volumes/Public/Container_Settings/sqlpage' \n",
    "\n",
    "# --- REPORT CONFIGURATION ---\n",
    "# Number of sample data rows to include in the schema report\n",
    "SAMPLE_ROWS = 3\n",
    "\n",
    "# --- EXPORT CONFIGURATION ---\n",
    "# The primary folder where reports will be saved (e.g., for GitHub)\n",
    "export_folder = '/Volumes/Public/Container_Settings/sqlpage/reports'\n",
    "\n",
    "# Second location to save reports (e.g., your Obsidian vault)\n",
    "obsidian_folder = \"/Users/david/Documents/remote/04 - Coding Project Docs\"\n",
    "\n",
    "# NEW: Subfolder name to use within the Obsidian vault\n",
    "obsidian_project_subfolder = \"SQLPage - Workout Logger\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905c0bfc",
   "metadata": {},
   "source": [
    "### Part A: Database Schema\n",
    "*These cells connect to the database and generate the schema report content.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3725550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded schema documentation for 1 tables.\n"
     ]
    }
   ],
   "source": [
    "# cell 3: YAML configuration\n",
    "# This cell contains the YAML configuration for the report.\n",
    "schema_docs = {}\n",
    "# Add this to your config\n",
    "schema_yaml_path = '/Volumes/Public/Container_Settings/sqlpage/schema'\n",
    "\n",
    "if os.path.isdir(schema_yaml_path):\n",
    "    for filename in os.listdir(schema_yaml_path):\n",
    "        # print(f\"Loading schema documentation from {filename}...\")\n",
    "        if filename.endswith('.yaml'):\n",
    "            with open(os.path.join(schema_yaml_path, filename), 'r') as f:\n",
    "                doc = yaml.safe_load(f)\n",
    "                if 'table_name' in doc:\n",
    "                    table_name = doc['table_name']\n",
    "                    schema_docs[table_name] = {\n",
    "                        'description': doc.get('description', '')}\n",
    "                    schema_docs[table_name]['columns'] = {\n",
    "                        col['name']: col.get('description', '')\n",
    "                        for col in doc.get('columns', []) if isinstance(col, dict) and 'name' in col\n",
    "                    }\n",
    "    print(f\"✅ Loaded schema documentation for {len(schema_docs)} tables.\")\n",
    "else:\n",
    "    print(\"⚠️ Schema YAML directory not found. Descriptions will be missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "65fc64d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 8 tables: sessions, _migrations, DimUser, DimDate, DimExercise, DimUserExercisePreferences, DimExercisePlan, FactWorkoutHistory\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Fetch Table Names and View Names\n",
    "\n",
    "table_names = []\n",
    "view_names = []  # New list to hold view names\n",
    "error_message = None\n",
    "\n",
    "if os.path.exists(db_path):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Fetch table names\n",
    "        cursor.execute(\n",
    "            \"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\"\n",
    "        )\n",
    "        table_names = [table[0] for table in cursor.fetchall()]\n",
    "\n",
    "        # Fetch view names\n",
    "        cursor.execute(\n",
    "            \"SELECT name FROM sqlite_master WHERE type='view' AND name NOT LIKE 'sqlite_%';\"\n",
    "        )\n",
    "        view_names = [view[0] for view in cursor.fetchall()]\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "        # Updated print statement\n",
    "        print(f\"✅ Found {len(table_names)} tables: {', '.join(table_names)}\")\n",
    "        if view_names:\n",
    "            print(f\"✅ Found {len(view_names)} views: {', '.join(view_names)}\")\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        error_message = f\"❌ Database error: {e}\"\n",
    "        print(error_message)\n",
    "else:\n",
    "    print(\"Skipping database processing because the path was not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8ce7fbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database connection opened.\n"
     ]
    }
   ],
   "source": [
    "## Cell 5: Initialize Report and Connect to Database\n",
    "# This cell prepares for report generation and opens the database connection.\n",
    "\n",
    "markdown_parts = []\n",
    "conn = None  # Initialize conn to None\n",
    "\n",
    "if table_names or view_names:\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        print(\"✅ Database connection opened.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"❌ Failed to connect to database: {e}\")\n",
    "        # conn will remain None on failure\n",
    "else:\n",
    "    print(\"Skipping database report generation as no tables or views were found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d1900b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Table of Contents generated.\n"
     ]
    }
   ],
   "source": [
    "## Cell 6: Generate Table of Contents\n",
    "# This cell builds the markdown table of contents for the report.\n",
    "\n",
    "if conn:  # Only proceed if the connection was successful\n",
    "    markdown_parts.append(\"\\n## Table of Contents\\n\")\n",
    "\n",
    "    # Add tables to TOC\n",
    "    if table_names:\n",
    "        markdown_parts.append(\"\\n**Tables**\\n\")\n",
    "        for table_name in sorted(table_names):\n",
    "            link_anchor = table_name.lower().replace(\"_\", \"-\")\n",
    "            markdown_parts.append(f\"- [{table_name}](#{link_anchor})\\n\")\n",
    "\n",
    "    # Add views to TOC\n",
    "    if view_names:\n",
    "        markdown_parts.append(\"\\n**Views**\\n\")\n",
    "        for view_name in sorted(view_names):\n",
    "            link_anchor = view_name.lower().replace(\"_\", \"-\")\n",
    "            markdown_parts.append(f\"- [{view_name}](#{link_anchor})\\n\")\n",
    "\n",
    "    print(\"✅ Table of Contents generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a69dfd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database Properties section generated.\n"
     ]
    }
   ],
   "source": [
    "## Cell 7: Generate Database Properties Summary\n",
    "# This cell adds a high-level summary of the database file's properties.\n",
    "\n",
    "if conn:\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Fetch database properties\n",
    "    cursor.execute(\"PRAGMA encoding;\")\n",
    "    encoding = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute(\"PRAGMA page_size;\")\n",
    "    page_size = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute(\"PRAGMA foreign_keys;\")\n",
    "    foreign_keys_status = \"On\" if cursor.fetchone()[0] == 1 else \"Off\"\n",
    "    \n",
    "    cursor.execute(\"PRAGMA journal_mode;\")\n",
    "    journal_mode = cursor.fetchone()[0].upper()\n",
    "\n",
    "    cursor.execute(\"PRAGMA user_version;\")\n",
    "    user_version = cursor.fetchone()[0]\n",
    "\n",
    "    # Assemble the markdown section\n",
    "    markdown_parts.append(\"\\n## Database Properties\\n\\n\")\n",
    "    properties_table = \"| Property | Value |\\n\"\n",
    "    properties_table += \"| :--- | :--- |\\n\"\n",
    "    properties_table += f\"| Encoding | {encoding} |\\n\"\n",
    "    properties_table += f\"| Page Size | {page_size} bytes |\\n\"\n",
    "    properties_table += f\"| Foreign Key Enforcement | {foreign_keys_status} |\\n\"\n",
    "    properties_table += f\"| Journal Mode | {journal_mode} |\\n\"\n",
    "    properties_table += f\"| User Version | {user_version} |\\n\"\n",
    "    \n",
    "    markdown_parts.append(properties_table)\n",
    "    \n",
    "    print(\"✅ Database Properties section generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e48b1acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modular helper functions are defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Define Helper Functions for Schema Details\n",
    "# This cell defines a set of modular functions to generate different parts of the schema report.\n",
    "\n",
    "# --- Individual Formatting Functions ---\n",
    "\n",
    "\n",
    "def format_column_details(object_name, db_connection, schema_docs):\n",
    "    \"\"\"Formats the 'Columns' table for a given object, including descriptions from YAML.\"\"\"\n",
    "    parts = [\"**Columns**\\n\\n\"]\n",
    "\n",
    "    # 1. ADDED \"Description\" to the markdown table header\n",
    "    parts.append(\n",
    "        \"| Name | Type | Not Null | Default | Primary Key | Description |\\n\")\n",
    "    parts.append(\"| :--- | :--- | :--- | :--- | :--- | :--- |\\n\")\n",
    "\n",
    "    cursor = db_connection.cursor()\n",
    "    cursor.execute(f\"PRAGMA table_info('{object_name}');\")\n",
    "\n",
    "    # 2. GET the specific column documentation for this table, with a fallback\n",
    "    table_column_docs = schema_docs.get(object_name, {}).get('columns', {})\n",
    "\n",
    "    for col in cursor.fetchall():\n",
    "        col_name = col[1]\n",
    "        name_md = f\"`{col_name}`\"\n",
    "\n",
    "        # Get the description for this specific column from the loaded docs\n",
    "        col_desc = table_column_docs.get(col_name, \"\")\n",
    "\n",
    "        col_type = col[2]\n",
    "        not_null = \"✅\" if col[3] == 1 else \"\"\n",
    "        default_val = f\"`{col[4]}`\" if col[4] is not None else \"\"\n",
    "        is_pk = \"✅\" if col[5] == 1 else \"\"\n",
    "\n",
    "        # 4. ADD the col_desc variable to the final table row\n",
    "        parts.append(\n",
    "            f\"| {name_md} | {col_type} | {not_null} | {default_val} | {is_pk} | {col_desc} |\\n\"\n",
    "        )\n",
    "\n",
    "    parts.append(\"\\n\")\n",
    "    return \"\".join(parts)\n",
    "\n",
    "\n",
    "def format_foreign_keys(table_name, db_connection):\n",
    "    \"\"\"Formats the 'Foreign Keys' table for a given table.\"\"\"\n",
    "    parts = []\n",
    "    cursor = db_connection.cursor()\n",
    "    cursor.execute(f\"PRAGMA foreign_key_list('{table_name}');\")\n",
    "    foreign_keys = cursor.fetchall()\n",
    "    if foreign_keys:\n",
    "        parts.append(\"**Foreign Keys**\\n\\n\")\n",
    "        parts.append(\"| Column | References Table | Foreign Column |\\n\")\n",
    "        parts.append(\"| :----- | :--------------- | :------------- |\\n\")\n",
    "        for fk in foreign_keys:\n",
    "            parts.append(f\"| `{fk[3]}` | `{fk[2]}` | `{fk[4]}` |\\n\")\n",
    "        parts.append(\"\\n\")\n",
    "    return \"\".join(parts)\n",
    "\n",
    "\n",
    "def format_indexes(table_name, db_connection):\n",
    "    \"\"\"Formats the 'Indexes' table for a given table.\"\"\"\n",
    "    parts = []\n",
    "    cursor = db_connection.cursor()\n",
    "    cursor.execute(f\"PRAGMA index_list('{table_name}');\")\n",
    "    indexes = cursor.fetchall()\n",
    "    if indexes:\n",
    "        parts.append(\"**Indexes**\\n\\n\")\n",
    "        parts.append(\"| Index Name | Columns | Unique |\\n\")\n",
    "        parts.append(\"| :--- | :--- | :--- |\\n\")\n",
    "        for index in indexes:\n",
    "            index_name = f\"`{index[1]}`\"\n",
    "            is_unique = \"✅\" if index[2] == 1 else \"\"\n",
    "            cursor.execute(f\"PRAGMA index_info('{index[1]}');\")\n",
    "            indexed_columns = [f\"`{info[2]}`\" for info in cursor.fetchall()]\n",
    "            columns_str = \", \".join(indexed_columns)\n",
    "            parts.append(f\"| {index_name} | {columns_str} | {is_unique} |\\n\")\n",
    "        parts.append(\"\\n\")\n",
    "    return \"\".join(parts)\n",
    "\n",
    "\n",
    "def format_triggers(table_name, db_connection):\n",
    "    \"\"\"Formats the 'Triggers' section for a given table.\"\"\"\n",
    "    parts = []\n",
    "    cursor = db_connection.cursor()\n",
    "    cursor.execute(\n",
    "        f\"SELECT name, sql FROM sqlite_master WHERE type='trigger' AND tbl_name='{table_name}';\"\n",
    "    )\n",
    "    triggers = cursor.fetchall()\n",
    "    if triggers:\n",
    "        parts.append(\"**Triggers**\\n\\n\")\n",
    "        for trigger_name, trigger_sql in triggers:\n",
    "            parts.append(f\"**Trigger:** `{trigger_name}`\\n\")\n",
    "            parts.append(f\"```sql\\n{trigger_sql}\\n```\\n\")\n",
    "        parts.append(\"\\n\")\n",
    "    return \"\".join(parts)\n",
    "\n",
    "\n",
    "def format_creation_sql(object_name, object_type_title, db_connection):\n",
    "    \"\"\"Formats the 'Creation SQL' section for a given object.\"\"\"\n",
    "    parts = []\n",
    "    cursor = db_connection.cursor()\n",
    "    db_object_type = object_type_title.lower()\n",
    "    cursor.execute(\n",
    "        f\"SELECT sql FROM sqlite_master WHERE type='{db_object_type}' AND name='{object_name}';\"\n",
    "    )\n",
    "    result = cursor.fetchone()\n",
    "    if result and result[0]:\n",
    "        parts.append(\"**Creation SQL**\\n\\n\")\n",
    "        parts.append(f\"```sql\\n{result[0]}\\n```\\n\")\n",
    "    return \"\".join(parts)\n",
    "\n",
    "\n",
    "def format_data_samples(object_name, db_connection, num_rows):\n",
    "    \"\"\"Formats the 'Data Samples' table for a given object.\"\"\"\n",
    "    parts = [f\"\\n**Data Samples (First {num_rows} Rows)**\\n\\n\"]\n",
    "    cursor = db_connection.cursor()\n",
    "    try:\n",
    "        cursor.execute(f'SELECT * FROM \"{object_name}\" LIMIT {num_rows};')\n",
    "        col_headers = [desc[0] for desc in cursor.description]\n",
    "        sample_data = cursor.fetchall()\n",
    "        if not sample_data:\n",
    "            parts.append(\"_Object is empty._\\n\\n\")\n",
    "        else:\n",
    "            parts.append(f\"| {' | '.join(col_headers)} |\\n\")\n",
    "            parts.append(f\"|{' :--- |' * len(col_headers)}\\n\")\n",
    "            for row in sample_data:\n",
    "                processed_row = [\n",
    "                    str(item) if item is not None else \"NULL\" for item in row\n",
    "                ]\n",
    "                parts.append(f\"| {' | '.join(processed_row)} |\\n\")\n",
    "            parts.append(\"\\n\")\n",
    "    except sqlite3.Error as e:\n",
    "        parts.append(f\"_Could not retrieve data: {e}_\\n\\n\")\n",
    "    return \"\".join(parts)\n",
    "\n",
    "\n",
    "# --- Main Orchestrator Function (with Error Handling) ---\n",
    "\n",
    "def generate_object_details(object_name, object_type_title, db_connection, schema_docs):\n",
    "    \"\"\"\n",
    "    Fetches and formats all details for a database object by calling specialized functions.\n",
    "    Catches errors for invalid or temporary objects.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        details_parts = [f\"\\n---\\n### {object_name}\\n\\n\"]\n",
    "\n",
    "        # Get the description. If the table or the description key\n",
    "        # doesn't exist, `table_desc` will be `None`, which is safe to check.\n",
    "        table_desc = schema_docs.get(object_name, {}).get('description')\n",
    "        if table_desc:\n",
    "            details_parts.append(f\"_{table_desc}_\\n\\n\")\n",
    "\n",
    "        # Pass schema_docs down to the next function\n",
    "        details_parts.append(format_column_details(\n",
    "            object_name, db_connection, schema_docs))\n",
    "\n",
    "        if object_type_title == \"Table\":\n",
    "            details_parts.append(format_foreign_keys(\n",
    "                object_name, db_connection))\n",
    "            details_parts.append(format_indexes(object_name, db_connection))\n",
    "            details_parts.append(format_triggers(object_name, db_connection))\n",
    "\n",
    "        details_parts.append(\n",
    "            format_creation_sql(object_name, object_type_title, db_connection)\n",
    "        )\n",
    "        details_parts.append(\n",
    "            format_data_samples(object_name, db_connection, SAMPLE_ROWS)\n",
    "        )\n",
    "\n",
    "        return \"\".join(details_parts)\n",
    "    except sqlite3.OperationalError as e:\n",
    "        # Gracefully handle errors for specific objects without crashing\n",
    "        print(\n",
    "            f\"⚠️  Skipping object '{object_name}' due to a processing error: {e}\")\n",
    "        error_report = [\n",
    "            f\"\\n---\\n### {object_name}\\n\\n\",\n",
    "            f\"_Could not generate documentation for this object. It may be temporary or invalid._\\n\\n\",\n",
    "            f\"**Error:** `{e}`\\n\",\n",
    "        ]\n",
    "        return \"\".join(error_report)\n",
    "\n",
    "\n",
    "print(\"✅ Modular helper functions are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "487eb894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Report body generated for all tables and views.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Generate Report Body for Tables and Views\n",
    "# This cell iterates through tables and views, calling the helper function to build the report content.\n",
    "\n",
    "if conn:\n",
    "    # Generate Details for Tables\n",
    "    if table_names:\n",
    "        markdown_parts.append(\"\\n## Table Schemas\\n\")\n",
    "        for table_name in sorted(table_names):\n",
    "            # FIX: Pass schema_docs to the function call\n",
    "            markdown_parts.append(generate_object_details(\n",
    "                table_name, \"Table\", conn, schema_docs))\n",
    "\n",
    "    # Generate Details for Views\n",
    "    if view_names:\n",
    "        markdown_parts.append(\"\\n## View Schemas\\n\")\n",
    "        for view_name in sorted(view_names):\n",
    "            # FIX: Pass schema_docs here as well\n",
    "            # Note: You may decide not to document views with YAML, but passing the variable is harmless.\n",
    "            markdown_parts.append(generate_object_details(\n",
    "                view_name, \"View\", conn, schema_docs))\n",
    "\n",
    "    print(\"✅ Report body generated for all tables and views.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0e3115be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Report content finalized and database connection closed.\n"
     ]
    }
   ],
   "source": [
    "## Cell 10: Finalize Report and Close Connection\n",
    "# This cell assembles the final report string and closes the database connection.\n",
    "\n",
    "schema_report_body = \"\"\n",
    "if conn:\n",
    "    schema_report_body = \"\".join(markdown_parts)\n",
    "    conn.close()\n",
    "    print(\"✅ Report content finalized and database connection closed.\")\n",
    "else:\n",
    "    print(\"Skipping report finalization as there was no database connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9b34ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 11: Helper Function for Saving\n",
    "# This cell defines a reusable function to handle saving files to prevent code duplication.\n",
    "\n",
    "\n",
    "def save_report(folder_path, filename, content):\n",
    "    \"\"\"A helper function to safely save content to a file.\"\"\"\n",
    "    if not folder_path:\n",
    "        print(\"_Skipping save because path is not set._\")\n",
    "        return\n",
    "    try:\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        with open(full_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "        print(f\"✅ Report successfully saved to:\\n'{full_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to save report to '{folder_path}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d12863cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Report successfully saved to:\n",
      "'/Volumes/Public/Container_Settings/sqlpage/reports/2025-07-02/2025-07-02 - SQLPage - Workout - Database Schema Report.md'\n",
      "\\nSaving copy to Obsidian folder...\n",
      "✅ Report successfully saved to:\n",
      "'/Users/david/Documents/remote/04 - Coding Project Docs/SQLPage - Workout Logger/reports/2025-07-02/2025-07-02 - SQLPage - Workout - Database Schema Report.md'\n"
     ]
    }
   ],
   "source": [
    "## Cell 12: Assemble and Save Schema Report\n",
    "# This cell assembles the final schema document with Obsidian frontmatter and saves it.\n",
    "\n",
    "if schema_report_body:\n",
    "    date_iso = datetime.date.today().isoformat()\n",
    "    \n",
    "    report_type = \"Database Schema Report\"\n",
    "    title = f\"{project_name} - {report_type}\"\n",
    "    filename = f\"{date_iso} - {title}.md\"\n",
    "    \n",
    "    summary = f\"Schema and data samples for the database powering the {project_name} application.\"\n",
    "\n",
    "    frontmatter = f\"\"\"---\n",
    "date: {date_iso}\n",
    "title: \"{title}\"\n",
    "summary: \"{summary}\"\n",
    "series: sqlpage.workout-logger\n",
    "github: https://github.com/drusho/SQLPage-Workout-Logger\n",
    "source: \"{db_path}\"\n",
    "categories: Homelab\n",
    "tags:\n",
    "  - sqlpage\n",
    "  - workout\n",
    "  - database\n",
    "  - schema\n",
    "  - documentation\n",
    "cssclasses:\n",
    "  - academia\n",
    "  - academia-rounded\n",
    "---\n",
    "\"\"\"\n",
    "    callout = f\"\"\">[!tip]+ Tip\n",
    "> - This report was auto-generated using the `SQLPage_Workout_Documentation_Generator.ipynb` notebook.\n",
    "\"\"\"\n",
    "    obsidian_content = frontmatter + callout + schema_report_body\n",
    "\n",
    "    # --- Version 2: Content for GitHub/Primary Export (New format) ---\n",
    "    github_h1_title = f\"{date_iso} - {report_type}\"\n",
    "    github_header = f\"# {github_h1_title}\\\\n\\\\n\"\n",
    "    github_summary = f'**summary:**\\\\n\"{summary}\"\\\\n\\\\n'\n",
    "    github_content = github_header + github_summary + callout + \"\\\\n\\\\n---\\\\n\" + schema_report_body\n",
    "\n",
    "    # --- SAVING LOGIC ---\n",
    "    # Save the GitHub version to the primary export folder\n",
    "    dated_export_folder = os.path.join(export_folder, date_iso)\n",
    "    save_report(dated_export_folder, filename, github_content)\n",
    "    \n",
    "    # If an Obsidian folder is specified, save the original version there\n",
    "    if obsidian_folder:\n",
    "        print(\"\\\\nSaving copy to Obsidian folder...\")\n",
    "        obsidian_reports_path = os.path.join(obsidian_folder, obsidian_project_subfolder, \"reports\", date_iso)\n",
    "        save_report(obsidian_reports_path, filename, obsidian_content)\n",
    "else:\n",
    "    print(\"Skipping save for schema report because no content was generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1ff87f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for schema drift (new tables or new columns)...\n",
      "✨ Schema check complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Generate or Update Schema YAML files\n",
    "\n",
    "if not os.path.isdir(schema_yaml_path):\n",
    "    print(f\"⚠️ Creating schema directory at: {schema_yaml_path}\")\n",
    "    os.makedirs(schema_yaml_path)\n",
    "\n",
    "if table_names:\n",
    "    print(\"Checking for schema drift (new tables or new columns)...\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Iterate through all tables found in the database\n",
    "    for table_name in sorted(table_names):\n",
    "        yaml_filename = f\"{table_name}.yml\"\n",
    "        yaml_filepath = os.path.join(schema_yaml_path, yaml_filename)\n",
    "\n",
    "        # Get the current list of columns for the table from the database\n",
    "        cursor.execute(f\"PRAGMA table_info('{table_name}');\")\n",
    "        db_columns_info = cursor.fetchall()\n",
    "        db_column_names = {col[1] for col in db_columns_info}\n",
    "\n",
    "        # --- CASE 1: The YAML file does NOT exist. Create it from scratch. ---\n",
    "        if not os.path.exists(yaml_filepath):\n",
    "            print(\n",
    "                f\"  -> New table found! Creating placeholder file: {yaml_filename}\")\n",
    "            yaml_structure = {\n",
    "                'table_name': table_name,\n",
    "                'description': 'ADD A DESCRIPTION FOR THIS TABLE',\n",
    "                'owner': 'NEEDS OWNER',\n",
    "                'tags': [],\n",
    "                'columns': [\n",
    "                    {'name': col[1], 'description': 'ADD A DESCRIPTION FOR THIS COLUMN', 'tests': [\n",
    "                    ]}\n",
    "                    # UPDATED: Sort columns alphabetically when creating a new file\n",
    "                    for col in sorted(db_columns_info, key=lambda c: c[1])\n",
    "                ]\n",
    "            }\n",
    "            with open(yaml_filepath, 'w', encoding='utf-8') as f:\n",
    "                yaml.dump(yaml_structure, f, sort_keys=False,\n",
    "                          default_flow_style=False, indent=2)\n",
    "\n",
    "        # --- CASE 2: The YAML file EXISTS. Check for new columns and update it. ---\n",
    "        else:\n",
    "            with open(yaml_filepath, 'r', encoding='utf-8') as f:\n",
    "                doc = yaml.safe_load(f) or {}\n",
    "\n",
    "            if 'columns' not in doc:\n",
    "                doc['columns'] = []\n",
    "\n",
    "            yaml_column_names = {col['name'] for col in doc.get(\n",
    "                'columns', []) if isinstance(col, dict) and 'name' in col}\n",
    "\n",
    "            new_columns = db_column_names - yaml_column_names\n",
    "\n",
    "            if new_columns:\n",
    "                print(\n",
    "                    f\"  -> Updating {yaml_filename} with {len(new_columns)} new column(s).\")\n",
    "                for col_name in sorted(list(new_columns)):\n",
    "                    doc['columns'].append({\n",
    "                        'name': col_name,\n",
    "                        'description': 'ADD A DESCRIPTION FOR THIS NEW COLUMN',\n",
    "                        'tests': []\n",
    "                    })\n",
    "\n",
    "                # UPDATED: Sort the entire list of columns alphabetically by name\n",
    "                doc['columns'].sort(key=lambda c: c.get('name', ''))\n",
    "\n",
    "                with open(yaml_filepath, 'w', encoding='utf-8') as f:\n",
    "                    yaml.dump(doc, f, sort_keys=False,\n",
    "                              default_flow_style=False, indent=2)\n",
    "\n",
    "    conn.close()\n",
    "    print(\"✨ Schema check complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b64ea",
   "metadata": {},
   "source": [
    "### Part B: Folder Tree\n",
    "*This cell generates the ASCII tree from the source folder.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a96f0d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully generated folder tree string.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Generate Folder Tree\n",
    "sqlpage_tree_string = \"\"\n",
    "\n",
    "\n",
    "def build_tree_recursive(dir_path, prefix=\"\"):\n",
    "    \"\"\"A recursive generator function to build the ASCII tree lines.\"\"\"\n",
    "    ignore_list = {\".DS_Store\", \"__pycache__\", \".git\", \".vscode\"}\n",
    "    try:\n",
    "        items = sorted(\n",
    "            [item for item in os.listdir(dir_path) if item not in ignore_list]\n",
    "        )\n",
    "    except OSError:\n",
    "        return\n",
    "    pointers = [\"├── \"] * (len(items) - 1) + [\"└── \"]\n",
    "    for pointer, item in zip(pointers, items):\n",
    "        yield prefix + pointer + item\n",
    "        path = os.path.join(dir_path, item)\n",
    "        if os.path.isdir(path):\n",
    "            extension = \"│   \" if pointer == \"├── \" else \"    \"\n",
    "            yield from build_tree_recursive(path, prefix + extension)\n",
    "\n",
    "\n",
    "if os.path.isdir(source_folder_for_tree):\n",
    "    root_name = os.path.basename(os.path.abspath(source_folder_for_tree))\n",
    "    tree_lines = [f\"{root_name}/\"]\n",
    "    tree_lines.extend(list(build_tree_recursive(source_folder_for_tree)))\n",
    "    sqlpage_tree_string = \"\\n\".join(tree_lines)\n",
    "    print(\"✅ Successfully generated folder tree string.\")\n",
    "else:\n",
    "    print(\n",
    "        f\"Skipping tree generation because the source folder '{source_folder_for_tree}' was not found.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d9b6f394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Report successfully saved to:\n",
      "'/Volumes/Public/Container_Settings/sqlpage/reports/2025-07-02/2025-07-02 - SQLPage - Workout - Folder Tree Report.md'\n",
      "\n",
      "Saving copy to Obsidian folder...\n",
      "✅ Report successfully saved to:\n",
      "'/Users/david/Documents/remote/04 - Coding Project Docs/SQLPage - Workout Logger/reports/2025-07-02/2025-07-02 - SQLPage - Workout - Folder Tree Report.md'\n"
     ]
    }
   ],
   "source": [
    "## Cell 15: Assemble and Save Folder Tree Report\n",
    "# This final cell assembles the folder tree report with frontmatter and saves it.\n",
    "\n",
    "if sqlpage_tree_string:\n",
    "    date_iso = datetime.date.today().isoformat()\n",
    "\n",
    "    report_type = \"Folder Tree Report\"\n",
    "    title = f\"{project_name} - {report_type}\"\n",
    "    filename = f\"{date_iso} - {title}.md\"\n",
    "\n",
    "    summary = f\"An ASCII tree representation of the file and folder structure for the {project_name} project.\"\n",
    "\n",
    "    frontmatter = f\"\"\"---\n",
    "date: {date_iso}\n",
    "title: \"{title}\"\n",
    "summary: \"{summary}\"\n",
    "series: sqlpage.workout-logger\n",
    "github: https://github.com/drusho/SQLPage-Workout-Logger\n",
    "source: \"{source_folder_for_tree}\"\n",
    "categories: Homelab\n",
    "tags:\n",
    "  - sqlpage\n",
    "  - workout\n",
    "cssclasses:\n",
    "  - academia\n",
    "  - academia-rounded\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "    callout = f\"\"\">[!tip]+ Tip\n",
    "> - This report was auto-generated using the `SQLPage_Workout_Documentation_Generator.ipynb` notebook.\n",
    "\"\"\"\n",
    "    tree_report_body = f\"## Directory Tree\\n\\n```tree\\n{sqlpage_tree_string}\\n```\"\n",
    "    full_content = frontmatter + callout + tree_report_body\n",
    "\n",
    "    dated_export_folder = os.path.join(export_folder, date_iso)\n",
    "    save_report(dated_export_folder, filename, full_content)\n",
    "\n",
    "    if obsidian_folder:\n",
    "        print(\"\\nSaving copy to Obsidian folder...\")\n",
    "        # NEW: Create the structured path for Obsidian\n",
    "        obsidian_reports_path = os.path.join(\n",
    "            obsidian_folder, obsidian_project_subfolder, \"reports\", date_iso\n",
    "        )\n",
    "        save_report(obsidian_reports_path, filename, full_content)\n",
    "else:\n",
    "    print(\"Skipping save for folder tree report because no content was generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c259aa",
   "metadata": {},
   "source": [
    "### Part C: SQL Documentation\n",
    "*These cells parse the Javadoc-style comments from your `.sql` files.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "496d7311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found and parsed documentation comments from 30 SQL files.\n"
     ]
    }
   ],
   "source": [
    "## Cell 16: Parse Comments from SQL Files\n",
    "\n",
    "sql_docs_data = []\n",
    "\n",
    "javadoc_regex = re.compile(r\"/\\*\\*(.*?)\\*/\", re.DOTALL)\n",
    "tag_regex = re.compile(r\"@(\\w+)\\s+(.*?)(?=\\s*@\\w+|\\s*$)\", re.DOTALL)\n",
    "\n",
    "if os.path.isdir(source_folder_for_tree):\n",
    "    for root, _, files in os.walk(source_folder_for_tree):\n",
    "        for file in sorted(files):\n",
    "            if file.endswith(\".sql\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "\n",
    "                    doc_block_match = javadoc_regex.search(content)\n",
    "                    if doc_block_match:\n",
    "                        doc_content = doc_block_match.group(1)\n",
    "\n",
    "                        tags_dict = defaultdict(list)\n",
    "                        raw_tags = tag_regex.findall(doc_content)\n",
    "\n",
    "                        for key, val in raw_tags:\n",
    "                            # Strip whitespace and remove leading asterisks\n",
    "                            cleaned_val = val.replace(\"*\", \"\").strip()\n",
    "                            # If the line starts with a hyphen (for list items), strip it\n",
    "                            if cleaned_val.startswith(\"-\"):\n",
    "                                cleaned_val = cleaned_val.lstrip(\"- \").strip()\n",
    "                            # Finally, normalize all remaining whitespace\n",
    "                            cleaned_val = \" \".join(cleaned_val.split())\n",
    "\n",
    "                            tags_dict[key].append(cleaned_val)\n",
    "\n",
    "                        cleaned_tags = dict(tags_dict)\n",
    "\n",
    "                        mtime_timestamp = os.path.getmtime(file_path)\n",
    "                        mtime_date = datetime.datetime.fromtimestamp(\n",
    "                            mtime_timestamp\n",
    "                        ).strftime(\"%Y-%m-%d\")\n",
    "                        cleaned_tags[\"file_last_modified\"] = [mtime_date]\n",
    "\n",
    "                        cleaned_tags[\"filepath\"] = [file_path]\n",
    "                        sql_docs_data.append(cleaned_tags)\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not process file {file_path}: {e}\")\n",
    "\n",
    "print(\n",
    "    f\"✅ Found and parsed documentation comments from {len(sql_docs_data)} SQL files.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "545028d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully generated full SQL documentation body.\n"
     ]
    }
   ],
   "source": [
    "## Cell 17: Generate SQL Documentation Body\n",
    "# This cell takes the parsed data from the previous cell and formats it into a readable markdown document body.\n",
    "\n",
    "sql_docs_body = \"\"\n",
    "if sql_docs_data:\n",
    "    # Sort the data by filepath to ensure a consistent order\n",
    "    sql_docs_data.sort(key=lambda x: x.get(\"filepath\", [\"\"])[0])\n",
    "\n",
    "    parts = [\"## SQL File Documentation\\n\"]\n",
    "    for doc in sql_docs_data:\n",
    "        # Helper function to safely get the first item from a list in the doc dict\n",
    "        def get_tag(tag_name, default=\"\"):\n",
    "            return doc.get(tag_name, [default])[0]\n",
    "\n",
    "        # --- Header and Metadata ---\n",
    "        filename = os.path.basename(get_tag(\"filepath\", \"N/A\"))\n",
    "        parts.append(f\"\\n---\\n### `{filename}`\\n\")\n",
    "        parts.append(f\"**Path:** `{get_tag('filepath')}`\\n\")\n",
    "\n",
    "        doc_updated = get_tag(\"last-updated\", \"N/A\")\n",
    "        file_modified = get_tag(\"file_last_modified\", \"N/A\")\n",
    "        date_status_emoji = \"✅\" if doc_updated == file_modified else \"⚠️\"\n",
    "        parts.append(\n",
    "            f\"**Last Updated (doc):** `{doc_updated}` | **File Modified:** `{file_modified}` {date_status_emoji}\\n\\n\"\n",
    "        )\n",
    "\n",
    "        # --- Core Description ---\n",
    "        parts.append(\n",
    "            f\"**Description:** {get_tag('description', '_No description provided._')}\\n\"\n",
    "        )\n",
    "\n",
    "        # --- Function to format list-based tags ---\n",
    "        def format_list_tag(tag_key, title):\n",
    "            if tag_key in doc:\n",
    "                parts.append(f\"\\n**{title}:**\\n\")\n",
    "                for item in doc[tag_key]:\n",
    "                    parts.append(f\"- {item}\\n\")\n",
    "\n",
    "        # --- Display all relevant tags in a clean order ---\n",
    "        format_list_tag(\"requires\", \"Requires\")\n",
    "        format_list_tag(\"param\", \"Parameters\")\n",
    "        format_list_tag(\"returns\", \"Returns\")\n",
    "        format_list_tag(\"see\", \"See Also\")\n",
    "        format_list_tag(\"note\", \"Notes\")\n",
    "        format_list_tag(\"todo\", \"TODO\")\n",
    "\n",
    "    sql_docs_body = \"\".join(parts)\n",
    "    print(\"✅ Successfully generated full SQL documentation body.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f59cea",
   "metadata": {},
   "source": [
    "### Part D: Project Style Guide\n",
    "*This final cell saves the standalone style guide document.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "04cec92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Report successfully saved to:\n",
      "'/Volumes/Public/Container_Settings/sqlpage/reports/2025-07-02/2025-07-02 - SQLPage - Workout - SQL Comment Documentation.md'\n",
      "\n",
      "Saving copy to Obsidian folder...\n",
      "✅ Report successfully saved to:\n",
      "'/Users/david/Documents/remote/04 - Coding Project Docs/SQLPage - Workout Logger/reports/2025-07-02/2025-07-02 - SQLPage - Workout - SQL Comment Documentation.md'\n"
     ]
    }
   ],
   "source": [
    "## Cell 18: Assemble and Save SQL Documentation Report\n",
    "# This final cell assembles the full document with frontmatter and saves your new SQL Documentation report.\n",
    "\n",
    "if sql_docs_body:\n",
    "    date_iso = datetime.date.today().isoformat()\n",
    "\n",
    "    report_type = \"SQL Comment Documentation\"\n",
    "    title = f\"{project_name} - {report_type}\"\n",
    "    filename = f\"{date_iso} - {title}.md\"\n",
    "\n",
    "    summary = \"A guide to docstring conventions and a summary of all documented SQL files in the project.\"\n",
    "\n",
    "    frontmatter = f\"\"\"---\n",
    "date: {date_iso}\n",
    "title: \"{title}\"\n",
    "summary: \"{summary}\"\n",
    "series: sqlpage.workout-logger\n",
    "github: https://github.com/drusho/SQLPage-Workout-Logger\n",
    "source: \"{source_folder_for_tree}\"\n",
    "categories: Homelab\n",
    "tags:\n",
    "  - sqlpage\n",
    "  - documentation\n",
    "  - style-guide\n",
    "cssclasses:\n",
    "  - academia\n",
    "  - academia-rounded  \n",
    "---\n",
    "\"\"\"\n",
    "    callout = f\"\"\">[!tip]+ Tip\n",
    "> - This report was auto-generated using the `SQLPage_Workout_Documentation_Generator.ipynb` notebook.\n",
    "\"\"\"\n",
    "\n",
    "    toc_parts = [\"\\n## Table of Contents\\n\\n\"]\n",
    "    for doc in sql_docs_data:\n",
    "        filename_in_toc = os.path.basename(doc.get(\"filepath\", [\"N/A\"])[0])\n",
    "        # CORRECTED: The .replace('.', '') has been removed to keep the file extension in the link.\n",
    "        link_anchor = filename_in_toc.lower()\n",
    "        toc_parts.append(f\"- [`{filename_in_toc}`](#{link_anchor})\\n\")\n",
    "    toc_body = \"\".join(toc_parts)\n",
    "\n",
    "    full_content = frontmatter + callout + toc_body + sql_docs_body\n",
    "\n",
    "    dated_export_folder = os.path.join(export_folder, date_iso)\n",
    "    save_report(dated_export_folder, filename, full_content)\n",
    "\n",
    "    if obsidian_folder:\n",
    "        print(\"\\nSaving copy to Obsidian folder...\")\n",
    "        obsidian_reports_path = os.path.join(\n",
    "            obsidian_folder, obsidian_project_subfolder, \"reports\", date_iso\n",
    "        )\n",
    "        save_report(obsidian_reports_path, filename, full_content)\n",
    "else:\n",
    "    print(\"Skipping save for SQL documentation because no content was generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
